\author{Group 40: Arnold, Gurunat, Huang, \& Qu}
\title{Modular Deep Encoder-Decoders}
\date{\today}
\documentclass[12pt]{article}
\usepackage{amsfonts}
\input{\string./core}

% Custom Title

%\def\maketitle{
    %\centering
    %\par\textbf{\LARGE\@title}
    %\par\hfill
    %\par{\@author, \@date}
    %\par\hfill
    %\par\hfill
    %\rule{\textwidth}{3pt}
%}

\def\maketitle{
    \begin{centering}
    \par\rule{\textwidth}{2pt}
    \par\hfill
    \par\textbf{\LARGE\@title}
    \par\hfill
    \par{\textit{\@author}}
    \par\hfill
    \par{\@date}
    \par\rule{\textwidth}{2pt}
    \end{centering}
}

\begin{document}
\thispagestyle{empty}
\maketitle
\hfill
\begin{abstract}
In this short paper we propose an approach to transfer learning using rich
vector embeddings. The suggested technique can be applied to any supervised
task, and it handles multiple sources and changing sources of data without the
need for retraining. To verify our ideas, we apply our ideas to the task of
text-classification.
\end{abstract}

\section{Introduction}\label{introduction}

Our goal is to generate rich vector embeddings from articles to classify
them into predefined categories. Then, we want to extend our model with
an additional source of data: the title of the videos. To handle this
new knowledge source without retraining our previous model, we suggest
to generate a new embedding that will be used to modify the original
one. This combined embedding will then be used for the classification
task.

More formally, we want to jointly learn a set of \emph{encoder}
functions $\{E_i\}_0^N$ mapping samples $x \sim \chi_i$ from a set of
data distributions $\{\chi_i\}_0^N$ to a fixed-sized vector embedding
$V$.

\[ \forall i \in [0; N]: E_i(x): \chi_i \rightarrow V_i \in \Re^M\]

\[V = \sum_i^N V_i\]

The embedding $V$ is then fed into a decoder function $D$ which in the
case of classification learns a mapping from the vector space of $V$ to
the label space $L$.

\[ D(v): \Re^M \rightarrow L \in \Re^D\]

Finally, we want to extend the set of encoders by learning a new encoder
$E_{N+1}$ which handles samples from a different dataset $\chi_{N+1}$,
without retraining our trained decoders and encoders.

\[ E_{N+1}(x): \chi_{N+1} \rightarrow V_{N+1} \in \Re^M\]

Although we could have used any kind of mapping, we chose to use deep
learning algorithms, as they easily learn hierarchical representations
and have been known to highly outperform other statistical techniques on
natural language tasks. Learning embeddings has been previously done by
\href{References}{{[}1{]}}, although our proposed contribution is
slightly different. \href{References}{{[}1{]}} used deep autoencoders to
obtain a better initialization of the parameters of their model. Our
approach instead is much closer in spirit to the work of Sutskever
\href{References}{{[}3{]}} and Vinyals \href{References}{{[}4{]}}. They
both use encoders on a sequence of data to generate a
\emph{thought-vector} which will then be decoded in the desired terget
sequence. In some sense, our proposal adds the transfer learning
component to their contribution. This approach is also similar to the
work of Karpathy \& al \href{References}{{[}5{]}} where they mapped
images to their captions with embeddings.

\section{Method}\label{method}
\subsection{Materials}
We used the freely available dataset from
\url{https://github.com/ParallelMazen/SaudiNewsNet} The dataset contains
a total of 31,030 Arabic newspaper articles, with title, author, date,
url and content in each entry. Article objects are reprensented in json
format, with UTF-8 encoding. We wrote a script to download the data from
github repo, unzip each file, and read them in as json objects. The
function can also give out content by key values such as title, author
and etc.

\subsection{Procedure}\label{procedure}

\subsubsection{Data Pre-processing and Features}

Firstly, the UTF-8 encoded data was checked for trailing spaces for removal. Next, the data was filtered by removing punctuation symbols, retaining only a subset consisting of \{$,$ $.$ $!$ $($ $)$ $?$\}. The data was then split into words which are used as tokens for building the vocabulary and finally obtaining a feature representation for training the neural network. A vocabulary is constructed by assigning unique integer word-ids to each unique word appearing in the corpora. The punctuations are treated as a separate token and contributes to the vocabulary. The url, date and title fields of the corpora were discarded since they were irrelevant for our purpose.  

After building the vocabulary for the corpora, the original sentences of the corpora are mapped on a word-basis to obtain a series of integers representing the sentence equivalent which form the features to be input to the embedding layer of the neural network. Alternatively, the integer mapping could be replaced with one-hot sparse encoding of each word constituting the sentence. The features are shuffled and split for training and testing. For efficient data storage and retrieval the features are compressed to HDF5 file format.

%\subsubsection{Neural Network Architecture}
\subsubsection{Embedding Layer}
An embedding layer is used to compute a word embedding $W: words \mapsto \mathbb{R}^{n}$ which is a function mapping words of a language to a high dimensional vectors which are continuous, distributed representation of the vocabulary words with good semantic properties. The hope of such a representation is to project similar words to similar regions in the hyper-space from a semantic point of view.

\subsubsection{Baseline System}
Two baseline systems are constructed, one to classify the journals based on the content of the articles and the second to classify the journals based on the authors of the articles. Identical architectures are used for both the baseline systems. The baseline system is a shallow neural network with one hidden Long Short Term Memory (LSTM) layer succeeding the embedding layer. The dimension of both the embedding layer and the LSTM was fixed to 128. The output layer is a soft-max layer with 14 dimension for each class representing each journal. Cross-Entropy loss function was used as an objective function to train the neural network using Adaptive-Gradient (Adagrad).

\subsubsection{Proposed System}
The procedure is quite straight-forward. While part of the team worked
on building our tailored dataset, the other half worked on the model
definition and training procedure.

Once every pre-requisite is available, we trained the first encoder
$E_1$ (implemented as a recurrent neural network) to build the embedding
$V_1$. Since we only dealt with a relatively simple classification task,
our decoder $D_1$ was simply a fully connected multi-layer perceptron.
They were jointly trained end-to-end by propagating the gradients
through the embedding from $D_1$ to $E_1$.

The second training step was to train the second encoder $E_2$. Again,
we also performed training end-to-end, but specifically did not
propagate the gradients through $E_1$.

\subsection{Evaluation}


\section{Results}\label{results}
\subsection{Baseline}
\subsection{Proposed System}
\section{Discussion}\label{discussion}

\section{References}\label{references}

Part of the relevant literature review. More literature was involved for
the deep learning part.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Supervised Representation Learning: Transfer Learning with Deep
  Autoencoders, Zhuang \& al,
  http://ijcai.org/Proceedings/15/Papers/578.pdf
\item
  Transfer Learning via Dimensionality Reduction, Pan \& al,

  https://www.cse.ust.hk/\textasciitilde{}jamesk/papers/aaai08.pdf
\item
  Sequence to Sequence Learning with Neural Networks, Sutskever \& al,

  http://arxiv.org/abs/1409.3215
\item
  Grammar as a Foreign Language, Vinyals \& al,
  http://arxiv.org/abs/1412.7449
\item
  Deep Fragment Embeddings for Bidirectional Image Sentence Mapping,
  Karpathy \& al, https://cs.stanford.edu/people/karpathy/nips2014.pdf
\end{enumerate}

%\end{multicols}
\end{document}
